---
title: "Solar Panels Analysis"
author: "Carly Lundgreen"
output: pdf_document
---


```{r echo=FALSE,include=FALSE,message=FALSE,warning=FALSE}
setwd("~/Grad_School/536")
library(ggplot2)
library(dplyr)
library(splines) #for b spline basis expansion
library(nlme) #for gls
library(astsa) #won't need this one unless I use sarima
library(MASS) #for stdres function
library(gridExtra)
library(kableExtra)
library(tidyverse)
library(utils) #for progress bar
source("https://raw.githubusercontent.com/MJHeaton/glstools/master/predictgls.R")
source("https://raw.githubusercontent.com/MJHeaton/glstools/master/stdres.gls.R")

#read data
solar <- read.csv("KWH.csv",header=TRUE,sep=",")
solar$Date <- as.Date(solar$Date)
```

# Introduction 

As the world moves further from fossil-fuel based sources of energy, research into the efficiency and viability of alternative sources of energy has become increasingly important. One particularly green source of energy is solar power, which is generated through solar panels. Solar panels are used everywhere from residential neighborhoods--solar panels can often be seen on rooftops in suburban areas--to massive solar farms that supply large quantities of energy to the power grid. 

The power that is generated by solar panels, measured in kilowatt hours (kWh), can vary widely due to factors such as the time of year, angle of the panels, direction of the sun, cloud cover, age of the panels, etc. Solar panels degrade over time, which means that the power output decreases over time as well. We have a dataset of daily power generated from a single solar panel system on a single customer household. We will analyze this data to help the power company understand how these solar panels are degrading over time, discover approximately how many years it takes for the panels to lose 50% of their average power generating capability, and obtain projections of power for the following year. 

This dataset has 1095 daily power measurements spanning three years: 2017, 2018, and 2019. We can see a clear seasonality to these data just from observing the time series plot below. Power generating capability appears to increase in the summer and decrease in the winter, on average. This is expected because there is much more sunlight in the summer than in the winter. Because solar power measurements are based on such a wide array of factors, a few of which were described above, it makes sense that there is a large variability in the individual power measurements. This is illustrated by the sharp increases and decreases in daily generated power. On average, however, we do see the clear seasonal trend of increasing average power in the warmer months and decreasing average power generated in the colder months. 

```{r echo=FALSE, fig.width=3.5, fig.height=2.5, fig.align="center"}
ggplot(data=solar, mapping=aes(x=Date, y=kwh)) + 
  geom_line(size=0.7,color="gray36") + 
  labs(x="Year", y="Power(kWh)") + 
  theme_bw() + 
  ggtitle("Solar Power Generated (2017-2019)") + 
  theme(plot.title = element_text(hjust = 0.5))
```

A particularly obvious problem with this dataset is that daily power measurements are not independent of each other. This is something we will have to incorporate into a statistical model to be able to sufficiently understand how these solar panels are degrading over time and obtain accurate predictions for future years. The Auto-Correlation Function (ACF) plot below displays the extent to which present observations are correlated with past observations in these data. Along the x-axis is a time lag that allows us to see how correlated observations are when they are $t_{1} - t_{0}$ days apart. 

```{r echo=FALSE, include=FALSE}
##Look at raw autocorrelations 
my.ACF <- acf(solar$kwh,main=NULL,lag.max=365,xlab="Lag (Days)",col="gray36") 
# you can see a definite seasonality about every 4 months (when the weather gets cold or warm)
```

```{r echo=FALSE,fig.width=3.5,fig.height=2.5,fig.align="center"}
acf_df <- data.frame(Lag=my.ACF$lag, ACF=my.ACF$acf)

ggplot(data=acf_df, aes(x=Lag, y=ACF)) + geom_col(color="gray36") + 
  labs(title="Autocorrelations",x="Lag (Days)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust=0.5))
```

The seasonality discussed previously is very obvious in this plot. This dataset begins on January 1st, so it appears that the correlation between observations changes after about 100 days. This is about 3 months, which means that observations in the spring and summer are still related to observations in January, only negatively so. The largest takeaway from this ACF plot, however, is that observations at successive time points are correlated, and this correlation takes a long time to "die out." We will attempt to capture this slowly-decaying correlation in the statistical model that we will outline in the following section. If the correlation is not accounted for, the standard errors associated with parameter estimates will be too small, which means that inference procedures such as confidence intervals on these parameters will be inaccurate. Predictions will also not be as accurate, because we are ignoring an important component the distribution of the response variable. 

# Model Specification 

In order to understand how the solar panels are degrading over time and predict future daily solar power measurements, we will fit a multiple linear regression model with a lag-1 autogressive covariance structure. The selected model is outlined below. We will compare this model to a linear model that includes Time as the only covariate and does not account for any correlation between successive observations, i.e. $y_{i} \sim N(X\beta,\sigma^2I)$. In following sections, we will outline where this linear model breaks down in the assumptions and how our model takes care of those issues. We will also compare evaluation metrics such as AIC, RMSE, and bias for the two models. 

The following model was fit to the solar panel data:


$$y_{i} = \beta_{0} + \beta_{1}x_{1} + \dots\ + \beta_{12}x_{12} + \epsilon,  \ \epsilon \sim N(0, \sigma^2R)$$

Where $y_{i}$ are daily solar power measurements taken from an individual home from the start of 2017 to the end of 2019. The variable $x_{1}$ is a numeric variable representing the date (year, month, and day) that the $i^{th}$ observation occurred. The rest of the columns of the design matrix **X** ($x_{2}, \dots, x_{12}$) are categorical variables indicating the month of the $i^{th}$ observation such that $x_{2}$ corresponds to whether the observation was taken in February, $x_{3}$ corresponds to whether the observation was taken in March, and so on through December. 

The design matrix is linear in the parameters $\beta_{0},\dots\,\beta_{12}$. Contextually, $\beta_{0}$ indicates the expected average daily power generated by these solar panels in January--which is the chosen "baseline" or comparison month for this particular model--and at time zero. Time equaling zero admittedly is impossible and uninterpretable, so we will refrain from further discussion of this parameter. 

We interpret $\beta_{1}$ as the expected average daily increase in power generated by these panels, holding all else constant. Because $\beta_{2},\dots,\beta_{12}$ are categorical indicators for each month, with January the default comparison month, they all have similar interpretations. For example, $\beta_{2}$ indicates how much more daily power in kWh is generated in February on average compared to January (holding all else constant) and $\beta_{3}$ indicates how much more daily power is generated in March on average compared to January, again holding all else constant. The remaining $\beta_{i}'s$ are interpreted similarly.

Regarding the uncertainty of this model, $\epsilon$ is a 1xn vector of differences between the actual solar power measurements and the power measurements predicted by the model for observations at times $t = 1,\dots\,n$. We also denote $\sigma^2$ as the residual variance, or the variance between observations. Essentially, $\sigma$ is the amount that we expect the model to differ from the true solar power measurements on average. 

**R** is an nxn matrix where the $ij^{th}$ elements are based on $\rho({t_{i}},{t_{j}})$, or the correlations between solar power measurements at time points $i, j$. We define $\rho(\epsilon_{t_{i}},\epsilon_{t_{j}})$ = $\phi^{\lvert t_{1} - t_{2}\rvert}$ so that the covariance matrix **R** has the following structure:

$$ \begin{pmatrix}
1 & \phi & \phi^2 & \dots & \phi^{t-1} \\
\phi & 1 & \phi & \vdots &  \phi^{t-2} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
\phi^{t-1} & \dots & \dots & \ddots & 1 \\
\end{pmatrix} $$

This covariance structure is referred to as a Lag-1 Autoregressive (AR1) process where measurements are correlated in time acccording to a parameter $\phi$, but the "further out" in time the measurements go, the correlation between observations decreases according to $\phi^{\lvert t_{1} - t_{2}\rvert}$, for $\phi \in \{-1,1\}$.

In this scenario, we assume that the diagonal elements of **R**, $\phi^{\lvert t_{i} - t_{i}\rvert}$, are equal to 1. We can make this assumption because the observations in this dataset were taken at uniformly varying time points (i.e. one measure of solar power taken per day). If we had a dataset with non-uniform time measurements, it may be possible to have more than one measurement of the response variable taken at the same time. In this situation, we would want to account for potential sampling variability between these "frozen time" measurements where the correlation is not necessarily equal to 1. This would introduce an alternative covariance structure that is different from the AR1 structure that we have chosen. It is important to recognize that our chosen structure works because we have uniform time measurements. 

# Model Justification 

```{r echo=FALSE}
#first, create numeric date variable 
numeric_dates <- function(date_vec){
  
yr <- character(length(date_vec))
mon <- character(length(date_vec))
day <- character(length(date_vec))

for (i in 1:length(date_vec)){
  test <- strsplit(as.character(date_vec[i]),split="-") %>% unlist()
  yr[i] <- test[1]
  mon[i] <- test[2]
  day[i] <- test[3]
}

yr <- as.numeric(yr)
month <- as.numeric(mon)
day <- as.numeric(day)

#create year month day variable 
yr + (month - 0.5)/12 + (day - 0.5)/365

}

#add ymd variable to dataframe
solar$ymd <- numeric_dates(solar$Date)
```

```{r echo=FALSE, fig.width=5,fig.height=3.5}
month_var <- function(date_vec){
  
yr <- character(length(date_vec))
mon <- character(length(date_vec))
day <- character(length(date_vec))

for (i in 1:length(date_vec)){
  test <- strsplit(as.character(date_vec[i]),split="-") %>% unlist()
  yr[i] <- test[1]
  mon[i] <- test[2]
  day[i] <- test[3]
}

yr <- as.numeric(yr)
month <- as.character(mon)
day <- as.numeric(day)

#create month variable
month

}

solar$month_c <- month_var(solar$Date)

solar$obs <- 1:nrow(solar) #this will come in handy for the GLS 

## GLS with AR(1) Errors
testgls <- gls(kwh~ymd + month_c,correlation=corAR1(form=~obs),data=solar,method="ML")

#standard linear model 
lm1 <- lm(kwh~Date,data=solar)

```

```{r echo=FALSE,include=FALSE}
#don't print this one
acf_lm1 <- acf(stdres(lm1),lag.max=365)

#compare to acf plot from only our linear model with main effect for month
no_corr_mod <- lm(kwh~ymd + month_c,data=solar)


acf_no_corr <- acf(stdres(no_corr_mod),lag.max=100,main="ACF (LM)",
                   xlab="Lag (Days)",col="gray36",cex.lab=0.95)

#model that accounts for correlation:
myacf2 <- acf(stdres.gls(testgls),lag.max=100,main="ACF (GLS)",
              xlab="Lag (Days)",cex.lab=0.95)
```

This model assumes that the residuals follow a normal distribution and that residuals vary equally about zero according to the constant variance $\sigma^2$. We are building the assumption of dependence into the model, so we depart from the typical linear regression assumption of independence between observations. First, however, we will plot the fitted values from our model aover the top of the observed solar power measurements. This plot is below.

```{r echo=FALSE, fig.width=4,fig.height=2.5,fig.align="center"}
ggplot(data=solar, mapping= aes(x=ymd, y = kwh)) + 
  geom_line(size=0.7) +labs(x="Time", y= "Power (kWh)",color="gray36") + 
  geom_line(aes(x=ymd, y=fitted(testgls)),size=1.06,color="red") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

```

We can see that our model, which is a step function that changes month-to-month, follows a similar undulating pattern that is seen in the solar power measurements. The model is predicting higher daily generated power in the spring and summer, which is just what we observe in the data.

In order to understand how our model captures the correlation in these data, we have displayed ACF plots for three models below. Model 1 is the ACF plot for the linear model with Time as the only covariate that does not account for any correlation. It is clear that successive days and months are correlated with previous days and months when using this model, which is expected since no correlation structure was incorporated. Model 2 was fit with the same design matrix as our selected model, but no correlation structure was included. It is clear that simply including an effect for month accounted for much of the correlation between observations, but there is still some lingering correlation effect that needed to be accounted for. Model 3 is our chosen model, outlined in the previous section. The ACF plot from our model shows that present observations are now only correlated with themselves and any lingering correlation over time has been explained by our AR1 structure. 

```{r echo=FALSE,fig.width=7,fig.height=2.5,}
acf1_df <- data.frame(Lag=acf_lm1$lag, ACF=acf_lm1$acf)
acf2_df <- data.frame(Lag=acf_no_corr$lag, ACF=acf_no_corr$acf)
acf3_df <- data.frame(Lag=myacf2$lag, ACF=myacf2$acf)

g1 <- ggplot(data=acf1_df, aes(x=Lag, y=ACF)) + geom_col(color="gray36") + 
  labs(title="Model 1",x="Lag (Days)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust=0.5))

g2 <- ggplot(data=acf2_df, aes(x=Lag, y=ACF)) + geom_col(color="gray36") + 
  labs(title="Model 2",x="Lag (Days)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust=0.5))

g3 <- ggplot(data=acf3_df, aes(x=Lag, y=ACF)) + geom_col(color="gray36") + 
  labs(title="Model 3",x="Lag (Days)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust=0.5))

grid.arrange(g1,g2,g3,ncol=3)
```

Now that we know that the residuals have been sufficiently "decorrelated" (i.e. the correlation sufficiently captured by our model), we will assess the assumptions of normality and equal variance among these decorrelated residuals. First, however, displayed below is a histogram of the standardized residuals from the simple linear model along with a plot of the fitted values vs. standardized residuals for this simple model. The histogram has a slight left skew and it is difficult to tell whether the assumption of normality is broken by this model, but there is a clear and obvious pattern in the fitted values vs. residuals plot. This is the same undulating pattern seen in the data, and is clear evidence that the linear model does not sufficiently meet the assumption of equal variance among the residuals. 

```{r echo=FALSE,fig.width=6.5,fig.height=2.5}
#check assumptions

##check normality of decorrelated residuals 
p1 <- ggplot()+geom_histogram(data=solar,mapping=aes(x=stdres.gls(testgls)),
                              color="white",fill="gray36",size=1,binwidth=.4) + 
  ggtitle("Standardized Residuals") + 
  labs(x="Decorrelated Residuals", y="Frequency") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

##check equal variance assumption decorrelated model
p2 <- ggplot(data=solar, mapping=aes(x=fitted(testgls),y=stdres.gls(testgls))) + 
  geom_point(color="gray36") + 
  geom_abline(slope=0, intercept=0,size=1,color="red") +
  ggtitle("Fitted Values vs. Residuals") + 
  labs(x="Fitted Values",y="Decorrelated & Std. Residuals") + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

#compare with plots from standard linear model 
p3 <- ggplot()+geom_histogram(data=solar,mapping=aes(x=stdres(lm1)),
                              color="white",fill="gray36",size=1,binwidth=.4) + 
  ggtitle("Standardized Residuals") + 
  labs(x="Std. Residuals", y="Frequency") + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) 

##check equal variance assumption from standard linear model
p4 <- ggplot(data=solar, mapping=aes(x=fitted(lm1),y=stdres(lm1))) + 
  geom_point(color="gray36") + 
  geom_abline(slope=0, intercept=0,size=1,color="red") +
  ggtitle("Fitted Values vs. Residuals") + 
  labs(x="Fitted Values",y="Std. Residuals") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
#plot correlated residuals (ie only with the linear model applied)
grid.arrange(p3,p4,ncol=2)
```

Contrasting the above plots with the two plots displayed below, we can see a clear improvement in the fitted values vs. residuals plot. The standardized (and decorrelated) residuals appear to vary constantly around zero, and the histogram of standardized residuals appears sufficiently normal to conclude that our model has met these assumptions. 

```{r echo=FALSE,fig.width=6.5,fig.height=2.5,fig.align="center"}
grid.arrange(p1,p2,ncol=2)
```

## Model Performance Evaluation 

```{r echo=FALSE}
r2_pseudo <- cor(solar$kwh, fitted(testgls))^2
```
In order to assess model fit, we calculated a "Pseudo $R^2$" using the following formula: $R^2_{pseudo} = Cor(y,fitted(mod))^2$. This value turned out to be 0.526. Ideally, this would be closer to 1, if we interpret this as a typical $R^2$ value, which is the percent of variation in the response variable explained by the model. Most likely this metric for model fit is catching on to the step-function aspect of our model. The variation in the solar power measurements may be better explained by a more smooth function. However, splines are not ideal due to the predictive goals of this analysis because splines do not behave well beyond the range of the observed data. We will assess predictive accuracy to get a better idea of how our model answers the prediction questions posed at the beginning of this analysis. 

We calculated AIC to begin assessing the predictive accuracy of our model, and the AIC was found to be 6388.822. This was much lower than the AIC for a simple linear model, which was found to be 8160.493. AIC is an estimator of prediction error (lower AIC quantities are better) and is mostly used as a means of model selection. If it wasn't already clear before, our model is much better at predicting than an SLR with a single Time covariate. 

The table below compares predictive accuracy metrics calculated from 30 cross-validated studies using a training set of 80% of the data. We are reporting the cross-validated root mean square error (RMSE), bias, average prediction interval coverage (i.e. what percent of prediction intervals actually contain the true value), and average prediction interval width for our model and for a simple linear regression (SLR) model that includes Time as the only covariate. We denote our model as "GLS" in the below table because our inclusion of a correlation structure required us to work in the generalized least squares framework rather than ordinary least squares. 

```{r echo=FALSE}
#create table of prediction metrics comparing my model w/ a regular linear model 
#the code for the cross-validation is in the file 'cross_val_stuff_correlated_data.R' 
#it took about 50 minutes to run so I ran it once and will use the output values in the below table: 

#cols: rpmse, bias, cvg, width 
#rows: GLS, LM

RMSE <- c(3.714,10.029)
Bias <- c(0.016,-0.066)
Coverage <- c(0.971, 0.957)
Width <- c(16.170, 39.416)

metrics <- data.frame(RMSE,Bias,Coverage,Width)
rownames(metrics) <- c("GLS","SLR")

mytab <- kable(metrics) %>% kable_styling(position="center",latex_options=c("HOLD_position")) %>%  row_spec(0,bold=TRUE) 

mytab1 <- column_spec(mytab,column=c(1),border_left=TRUE,bold=TRUE)
column_spec(mytab1,column=c(5),border_right=TRUE) 

```

The RMSE of our model is clearly lower than that of the SLR model. Our RMSE of 3.7, indicating we are off in predictions by 3.7 kwH on average, is much smaller than the interquartile range of the solar power measurements, which is 14.5. Thus the predictive accuracy of the model is excellent. The bias for the two models is comparable in magnitude, and the bias of 0.016 from our model means we overpredict, on average, by this amount. The linear model, on average, underpredicts by a slightly greater magnitude. This bias is very small in comparison to the spread of the solar power measurements, indicating excellent predictive accuracy. The average prediction interval width of our model is slightly larger than the IQR, which is relatively large in comparison to the spread of the response variable. However, there is a large spread in the observed solar power values from 2017-2019, so the standard errors are likely not very far off what we would expect to see in comparison to what we have already observed. The coverage of 0.97 means that 97% of all the 95% prediction interval widths calculated actually contained the true observed value. This is close to 0.95, so it appears that our model is working as expected and predicting quite accurately. 

# Results 

The table below displays estimates of each of the $\beta_{i}$ values discussed in the Model Selection section, along with 95% confidence interval bounds. 

```{r echo=FALSE,message=FALSE,warning=FALSE}
#predictions

#get dates of next few years
st <- as.Date("2020-01-01")
en <- as.Date("2028-12-31")
new_date_seq <- seq(from=st, to=en, by="day") 
new_dates <- new_date_seq %>% numeric_dates() %>% as.data.frame()
colnames(new_dates) <- c("ymd")
end_obs <- 1095 + nrow(new_dates)
new_dates$obs <- seq(1096:end_obs)
new_dates$month_c <- month_var(new_date_seq)
myPreds <- predictgls(testgls, newdframe = new_dates, level=0.95)
```

```{r echo=FALSE}
ests <- data.frame(low = confint(testgls)[,1], est = testgls$coefficients, up = confint(testgls)[,2])
est_df <- apply(ests,2,round,digits=3)
rownames(est_df) <- c("Intercept","Date",month.name[-1])
colnames(est_df) <- c("2.5%", "Estimate", "97.5%")

tab1 <-  kable(est_df) %>% kable_styling(position="center",latex_options=c("HOLD_position")) %>%  row_spec(0,bold=TRUE) 

mytab1 <- column_spec(tab1,column=c(1),border_left=TRUE,bold=TRUE)
column_spec(mytab1,column=c(4),border_right=TRUE) 
```

 Based on these estimates, we conclude the following: 

We can expect between a `r est_df[2,3]*-1` and `r est_df[2,1]*-1` kWh average daily decrease in generated power over time, assuming other factors are held constant. It is clear that the solar panels are degrading over time because the average daily power generated decreases over time. 

Below is a plot displaying the solar power data along with our model's predicted average daily power measurements in kWh for the years 2020-2028. The red lines indicate the standard errors of our predicted power measurements, and we can see that the measurements become more uncertain as time goes on. It seems that the average maximum power generated each year (which appears to be in the middle of the year for all the predicted years) decreases over time. The top horizontal blue line indicates the maximum average daily power generated for the year 2017, based on the data and our model. This maximum appears to be about 40 kWh per day. The bottom blue line indicates the maximum average daily power generated in 2028, and this maximum appears to be about 20 kWh per day. Based on this, it appears that it takes approximately 11 years for the panels to lose 50% of their average power generating capability.

```{r echo=FALSE,message=FALSE,warning=FALSE,fig.width=4,fig.height=2.5,fig.align="center"}

myPreds$indicator <- c("pred")
myPreds$kwh <- myPreds$Prediction
myPreds2 <- myPreds[,c("ymd","obs","kwh","lwr","upr","indicator")]

### combine datasets
solar$indicator <- c("actual")
solar$lwr <- NA
solar$upr <- NA
solar2 <- solar[,c("ymd","obs","kwh","lwr","upr","indicator")]

combined <- rbind(solar2,myPreds2) %>% as.data.frame()
combined$fitted <- c(fitted(testgls),rep(NA,nrow(combined)-length(fitted(testgls))))

ggplot(data=combined, mapping=aes(x=ymd, y=kwh)) + 
  geom_line(size=0.7,color="gray37") + 
  geom_line(aes(x=ymd,y=fitted),color="black",size=1) + 
  labs(x="Year", y="Power (kwh)") + 
  theme_bw() + 
  geom_line(aes(x=ymd,y=lwr),color="red") + 
  geom_line(aes(x=ymd,y=upr),color="red") + 
  geom_hline(yintercept=43,color="blue",lty=2,size=1) + 
  geom_hline(yintercept=21.5,color="blue",lty=2,size=1) 

```

Though in the plot above we can see the predicted average daily power generated by these solar panels for the year 2020, the table below displays the average predicted daily generated power for each month of 2020 (the year just following our dataset). Because these are averages of predictions, they are in a sense "averages of averages", but they provide a snapshot of how much power these solar panels will generate for the next year. Average 95% prediction interval bounds are also included to get a sense of the uncertainty around each estimate. 

```{r echo=FALSE}
summaries <- group_by(myPreds,month_c) %>%
              summarise (
                Low = mean(lwr) %>% round(3),
                Estimate = mean(Prediction) %>% round(3),
                Up = mean(upr) %>% round(3))
df2 <- summaries[,-1] %>% as.data.frame()
rownames(df2) <- month.name
colnames(df2) <- c("2.5%","Estimate","97.5%")

tab1 <-  kable(df2) %>% kable_styling(position="center",latex_options=c("HOLD_position")) %>%  row_spec(0,bold=TRUE) 

mytab1 <- column_spec(tab1,column=c(1),border_left=TRUE,bold=TRUE)
column_spec(mytab1,column=c(4),border_right=TRUE) 

```

# Conclusion 

By modeling power by time along with a categorical month effect, as well as including a lag-1 autoregressive covariance structure, we were able to capture the variation in solar power measurements along with the correlation between successive observations. The estimates of $\beta$, the vector of effects of each of the covariates in the model, provide an understanding of how the solar panels are degrading over time. We projected that it will take around 11 years for the solar panels to lose 50% of their power generating capability. The relevance of this projection depends on factors such as how much the panels cost to install/replace, how much money is actually saved by having the solar panels installed, and how well these panels perform relative to panels from other companies.

Though the model is able to predict quite accurately, it does have a shortcoming in the quality of the model fit. Using a "Pseudo $R^2$", we found that our covariates (linear time variable and categorical month variable) explained about 53% of the variation in generated power under the traditional $R^2$ interpretation. This could potentially be improved by including other basis function expansions in **X** and running a Lasso regression as a means of variable selection. Another option is to use only Time as the covariate and apply a more complicated correlation structure that accounts for the obvious summer-winter seasonality that shows up in this data. Also, these data were gathered from a single customer household, which probably does not give the power company the ability to generalize these results to their entire population of customers. Data from more households would provide more insight into the function of the actual product because variation attributed to individual homes would be ironed out as the sample size (of homes) became sufficiently large.

A final point of further analysis may be to gather outside covariates that might help explain sunlight (and thus the amount of power generated by the solar panels) to see how variables other than time, and basis expansions of time, affect the generating power of these solar panels. This would allow the customer to get further insight into the financial benefit that the solar panels provide and give the power company insight into how well the solar panels actually function. 

